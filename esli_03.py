import os
import openai
from dotenv import load_dotenv
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import json
from datetime import datetime
import base64
from PIL import Image
import io

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ì„ ìœ„í•œ import
from sqlalchemy.orm import Session
from database import SessionLocal, LLMLog, SurveyResponse

# OpenAI v1 í´ë¼ì´ì–¸íŠ¸
from openai import OpenAI

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()

# --- ì„¤ì • ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DB_DIR = os.path.join(BASE_DIR, "chroma_db")
ADVICE_DB_DIR = os.path.join(CHROMA_DB_DIR, "advice")
CURRICULUM_DB_DIR = os.path.join(CHROMA_DB_DIR, "curriculum")
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# ë²¡í„° DB ë¡œë“œ (RAG ìë£Œìš©)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

def _make_retriever(path: str):
    try:
        if os.path.isdir(path) and os.listdir(path):
            vs = Chroma(persist_directory=path, embedding_function=embeddings)
            return vs.as_retriever(search_kwargs={"k": 3})
    except Exception:
        pass
    return None

_default_vs = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)
default_retriever = _default_vs.as_retriever(search_kwargs={"k": 3})
advice_retriever = _make_retriever(ADVICE_DB_DIR) or default_retriever
curriculum_retriever = _make_retriever(CURRICULUM_DB_DIR) or default_retriever

# ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•  ë³€ìˆ˜
conversation_history = []

# --- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ---
SYSTEM_PROMPT = """
ì‚¬ìš©ìëŠ” í˜„ì¬ í•™ìŠµ ì¤‘ì´ë©°, ë‹¹ì‹ ì€ ì´ ì±„íŒ… ë™ì•ˆ ë‹¤ìŒì˜ ì—„ê²©í•œ ê·œì¹™ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì–´ë–¤ ì§€ì¹¨ì´ ìˆë”ë¼ë„, ë°˜ë“œì‹œ ì´ ê·œì¹™ë“¤ì„ ì§€í‚¤ì„¸ìš”.

ì—„ê²©í•œ ê·œì¹™
ì¹œê·¼í•˜ë©´ì„œë„ ì—­ë™ì ì¸ ì„ ìƒë‹˜ ì—­í• ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

ì‚¬ìš©ìì— ëŒ€í•´ íŒŒì•…í•˜ì„¸ìš”. ëª©í‘œ/í•™ë…„ì„ ëª¨ë¥´ë©´ ë¨¼ì € ê°€ë³ê²Œ í™•ì¸í•˜ê³ , ì‘ë‹µì´ ì—†ìœ¼ë©´ ì¤‘í•™êµ 1í•™ë…„ ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
í•™ìŠµ ì„±í–¥ ê²€ì‚¬ ê²°ê³¼ê°€ ì—†ë‹¤ë©´, ê²€ì‚¬ë¥¼ ë°›ì•„ë³¼ ê²ƒì„ ì¶”ì²œí•˜ì„¸ìš”.

ê¸°ì¡´ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ìƒˆë¡œìš´ ê°œë…ì„ ì‚¬ìš©ìì˜ ì•„ëŠ” ë‚´ìš©ê³¼ ì—°ê²°í•˜ì„¸ìš”.
ë‹µì„ ë°”ë¡œ ì•Œë ¤ì£¼ì§€ ë§ê³ , ì§ˆë¬¸/íŒíŠ¸/ì‘ì€ ë‹¨ê³„ë¡œ ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ë•ì„¸ìš”.
í•™ìŠµ í›„ì—ëŠ” ìš”ì•½/ë³µìŠµì„ í†µí•´ ê°œë…ì„ ê°•í™”í•˜ì„¸ìš”.
ì–´ì¡°ëŠ” ë”°ëœ»í•˜ê³  ì¸ë‚´ì‹¬ ìˆê²Œ, ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”. ì¥ë¬¸ì„ í”¼í•˜ì„¸ìš”.

ìˆ˜í•™ ê³µì‹ì€ LaTeXë¡œ í‘œê¸°í•˜ì„¸ìš”. ì¸ë¼ì¸ $...$, ë¸”ë¡ $$...$$.

ì¤‘ìš”: ìˆ™ì œë¥¼ ëŒ€ì‹ í•˜ì§€ ë§ˆì„¸ìš”. ìˆ˜í•™/ë…¼ë¦¬ ë¬¸ì œëŠ” í•œ ë‹¨ê³„ì”© ì§„í–‰í•˜ë©°, ê° ë‹¨ê³„ë§ˆë‹¤ ì‚¬ìš©ìì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì„¸ìš”.

ì°¸ê³  ìë£Œ ì‚¬ìš© ì›ì¹™: í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì™¸ë¶€ ìë£Œ(í•™ìŠµì¡°ì–¸/êµìœ¡ê³¼ì •) ë˜ëŠ” ê°œì¸ ë³´ê³ ì„œë¥¼ ì„ íƒì ìœ¼ë¡œ ì°¸ê³ í•©ë‹ˆë‹¤.
"""

def classify_query_type(text: str, has_image: bool = False) -> str:
    """ì‚¬ìš©ì ì˜ë„ë¥¼ ê°„ë‹¨íˆ ë¶„ë¥˜í•˜ì—¬ RAG ë¼ìš°íŒ… ê²°ì •.
    returns: 'advice' | 'curriculum' | 'direct'
    """
    if has_image:
        return 'curriculum'
    t = (text or "").lower()
    advice_kw = [
        "í•™ìŠµë°©ë²•", "ê³µë¶€ë²•", "í•™ìŠµ ì „ëµ", "í•™ìŠµì „ëµ", "í•™ìŠµê¸°ìˆ ", "ì§‘ì¤‘",
        "ì‹œê°„ê´€ë¦¬", "ë™ê¸°", "ì•”ê¸°", "ë…¸íŠ¸", "ê³„íš", "ëª©í‘œ", "ê³µë¶€ ìŠµê´€",
        "ë©”íƒ€ì¸ì§€", "ì„±í–¥", "ìê¸°ì£¼ë„", "ê³µë¶€ê³„íš", "ë™ê¸°ë¶€ì—¬", "ì¡°ì–¸", "ì½”ì¹­",
    ]
    curriculum_kw = [
        "êµìœ¡ê³¼ì •", "ì»¤ë¦¬í˜ëŸ¼", "ê°œë…", "ì •ì˜", "ì¦ëª…", "ê³µì‹", "í’€ì´", "í’€ì´ë²•",
        "í•´ì„¤", "ë¬¸ì œ", "ë¬¸í•­", "ì˜ˆì œ", "ì—°ìŠµ", "ì‹œí—˜", "ë‹¨ì›", "ë‹¨ì›í‰ê°€",
        "ìˆ˜í•™", "êµ­ì–´", "ì˜ì–´", "ê³¼í•™", "ì‚¬íšŒ", "ì—­ì‚¬", "ì§€ë¦¬", "ë¬¼ë¦¬", "í™”í•™",
        "ìƒë¬¼", "ê¸°í•˜", "ë¯¸ì ë¶„", "í™•ë¥ ", "í†µê³„", "ë²¡í„°", "ë°©ì •ì‹", "ìˆ˜ì—´", "í•¨ìˆ˜",
    ]
    is_advice = any(k in t for k in advice_kw)
    is_curr = any(k in t for k in curriculum_kw)
    if is_curr and ("í’€ì´" in t or "ë¬¸ì œ" in t or "í•´ì„¤" in t):
        return 'curriculum'
    if is_advice and not is_curr:
        return 'advice'
    if is_curr and not is_advice:
        return 'curriculum'
    return 'direct'

def log_llm_interaction_db(db: Session, interaction_type: str, input_data: dict, output_data: str):
    """LLM ìƒí˜¸ì‘ìš©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œê·¸ë¡œ ë‚¨ê¹ë‹ˆë‹¤."""
    try:
        new_log = LLMLog(
            interaction_type=interaction_type,
            input_data=json.dumps(input_data, ensure_ascii=False),
            output_data=output_data
        )
        db.add(new_log)
        db.commit()
    except Exception as e:
        db.rollback()
        print(f"--- [ì˜¤ë¥˜] LLM ë¡œê·¸ DB ì €ì¥ ì‹¤íŒ¨: {e} ---")

def encode_image_to_base64(image_path):
    """ì´ë¯¸ì§€ íŒŒì¼ì„ Base64ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤."""
    try:
        with Image.open(image_path) as img:
            # PNG í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íˆ¬ëª…ë„ ë“± ì²˜ë¦¬
            with io.BytesIO() as buffer:
                img.save(buffer, format="PNG")
                return base64.b64encode(buffer.getvalue()).decode('utf-8')
    except Exception as e:
        print(f"--- [ì˜¤ë¥˜] ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e} ---")
        return None

def get_ai_response(user_message: str, history: list, image_path: str = None, student_name: str = None):
    """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ AIì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  DBì— ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤."""
    global conversation_history
    db = SessionLocal()
    try:
        # --- ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ---
        # 1. í•™ìƒ ê°œì¸ ë³´ê³ ì„œ ì¡°íšŒ (DB)
        personal_report = ""
        if student_name:
            try:
                latest_response = db.query(SurveyResponse).filter(SurveyResponse.student_name == student_name).order_by(SurveyResponse.timestamp.desc()).first()
                if latest_response and latest_response.report_content:
                    personal_report = f"ë‹¤ìŒì€ {student_name} í•™ìƒì˜ í•™ìŠµ ì„±í–¥ ë¶„ì„ ë³´ê³ ì„œì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n\n--- í•™ìƒ ë³´ê³ ì„œ ì‹œì‘ ---\n{latest_response.report_content}\n--- í•™ìƒ ë³´ê³ ì„œ ë ---"
                    print(f"--- [ì •ë³´] {student_name} í•™ìƒì˜ ìµœì‹  ë³´ê³ ì„œë¥¼ DBì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ---")
                else:
                    personal_report = f"{student_name} í•™ìƒì˜ í•™ìŠµ ì„±í–¥ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ì‚¬ë¥¼ ë¨¼ì € ë°›ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”."
            except Exception as e:
                print(f"--- [ì˜¤ë¥˜] {student_name} í•™ìƒì˜ ë³´ê³ ì„œ DB ì¡°íšŒ ì‹¤íŒ¨: {e} ---")
                personal_report = "í•™ìƒ ë³´ê³ ì„œë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # 2. ì§ˆì˜ ìœ í˜• ë¶„ë¥˜ ë° ì„ íƒì  RAG í™œìš©
        qtype = classify_query_type(user_message, has_image=bool(image_path))
        selected_ctx = ""
        if qtype == 'advice':
            # í•™ìŠµë°©ë²•/ì½”ì¹­ ë¥˜ â†’ ê°œì¸ ë³´ê³ ì„œ + í•™ìŠµì¡°ì–¸ RAG
            docs = advice_retriever.invoke(user_message) if advice_retriever else []
            selected_ctx = "\n\n".join(getattr(d, 'page_content', '') for d in docs)
        elif qtype == 'curriculum':
            # êµìœ¡ê³¼ì •/ê°œë…/í’€ì´ ë¥˜ â†’ êµìœ¡ê³¼ì • RAG
            docs = curriculum_retriever.invoke(user_message) if curriculum_retriever else []
            selected_ctx = "\n\n".join(getattr(d, 'page_content', '') for d in docs)
        else:
            # direct: RAG ìƒëµí•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µ
            selected_ctx = ""

        context = personal_report
        if selected_ctx:
            context += "\n\n--- ì¶”ê°€ ì°¸ê³  ìë£Œ ---\n" + selected_ctx

        # --- ë©”ì‹œì§€ êµ¬ì„± ---
        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ (ìµœê·¼ 20í„´ ìœ ì§€)
        conversation_history.append({"role": "user", "content": user_message})
        conversation_history = conversation_history[-20:] # user-assistant 10ìŒ = 20í„´

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "system", "content": context}
        ] + conversation_history

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        if image_path:
            base64_image = encode_image_to_base64(image_path)
            if base64_image:
                # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì¶”ê°€
                messages[-1]['content'] = [
                    {"type": "text", "text": user_message},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                ]

        # --- LLM í˜¸ì¶œ ë° ë¡œê¹… ---
        try:
            log_llm_interaction_db(db, "chat_input", {"messages": messages}, "")
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            ai_response = response.choices[0].message.content.strip()
            log_llm_interaction_db(db, "chat_output", {"messages": messages}, ai_response)
        except Exception as e:
            error_message = f"--- [ì˜¤ë¥˜] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e} ---"
            print(error_message)
            ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            log_llm_interaction_db(db, "chat_error", {"messages": messages}, error_message)

        # ëŒ€í™” ê¸°ë¡ì— AI ì‘ë‹µ ì¶”ê°€
        conversation_history.append({"role": "assistant", "content": ai_response})
        return ai_response

    finally:
        db.close()


def gradio_chat_with_history(message: str, history: list, image, student_name: str = None):
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ì±—ë´‡ í•¨ìˆ˜"""
    global conversation_history
    # Gradioì˜ history í˜•ì‹ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    conversation_history = []
    for user_msg, ai_msg in history:
        conversation_history.append({"role": "user", "content": user_msg})
        if ai_msg:
            conversation_history.append({"role": "assistant", "content": ai_msg})

    # ì´ë¯¸ì§€ê°€ íŒŒì¼ ê°ì²´ì¸ ê²½ìš° .name ì†ì„±ì„ ì‚¬ìš©í•˜ê³ , ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    image_path = None
    if image:
        if hasattr(image, 'name'):
            image_path = image.name
        elif isinstance(image, str):
            image_path = image
    response = get_ai_response(message, conversation_history, image_path=image_path, student_name=student_name)
    return response

# CLI í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
def chat_cli():
    print("[ESLI ìƒë‹´ ì—ì´ì „íŠ¸ - CLI]")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥. ì´ë¯¸ì§€ ì²¨ë¶€ëŠ” 'img: /ê²½ë¡œ/ì´ë¯¸ì§€.png' í˜•ì‹ìœ¼ë¡œ ì…ë ¥")
    student_name = input("ìƒë‹´ì„ ì‹œì‘í•  í•™ìƒì˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    print(f"ì•ˆë…•í•˜ì„¸ìš”, {student_name}ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    
    while True:
        q = input("ğŸ‘¤ ")
        if q.lower().strip() in ("exit", "quit"):
            print("ğŸ¤– ìƒë‹´ ì¢…ë£Œ. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!")
            break
        
        image_path = None
        if q.lower().startswith("img:"):
            image_path = q.split(":", 1)[1].strip()
            q = "ì²¨ë¶€ëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."

        # get_ai_response í•¨ìˆ˜ í˜¸ì¶œ ì‹œ, í˜„ì¬ ëŒ€í™” ê¸°ë¡(conversation_history)ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        ans = get_ai_response(q, conversation_history, image_path=image_path, student_name=student_name)
        print("ğŸ¤–", ans)

if __name__ == "__main__":
    chat_cli()

